{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task is to break down hashtags into proper words. #GoldenGlobe -> Golden, Globe\n",
    "Let's first bring the data into a list. Each line/tweet is a item in a list. Further, we will treat them as a document comprising of some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadTweetsInList(filename):\n",
    "    tweets = open(filename, \"rU\")\n",
    "    message = tweets.readlines()\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = loadTweetsInList(\"../as.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['American Sniper\\n',\n",
       " \"If American Sniper doesn't make you proud to be an American, you probably need to check your pulse. Merica. http://t.co/d2bkKzbfWA\\n\",\n",
       " \"Retweet if you're going to see the movie American Sniper http://t.co/8Q93NiAWJJ\\n\",\n",
       " \"Retweet if you're going to see the movie American Sniper http://t.co/AQAtAo3vep\\n\",\n",
       " \"Retweet if you're going to see the movie American Sniper http://t.co/CYdwIZxcSW\\n\",\n",
       " \"Fav if you're going to see the movie American Sniper! http://t.co/DxWI1Sq6Jw\\n\",\n",
       " 'I just watched American Sniper. You have to see what Bradley Cooper does in this movie. His performance is next level.\\n',\n",
       " 'Did you know #Jesseventura sued fallen seal estate #AmericanSniper for $1.8M. You worthless piece of \\xf0\\x9f\\x92\\xa9. Shameless.\\n',\n",
       " \"American Sniper kind of reminds me of the movie that's showing in the third act of Inglorious Basterds.\\n\",\n",
       " 'Did y\\xe2\\x80\\x99all see \"American Sniper\"? What a powerful movie. Bradley Cooper isn\\xe2\\x80\\x99t too hard on the eyes, either.\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets drop first item since its just a title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = tweets[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets tokenize each string in this list. The result should be list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def cleanAndTokenizeTweet(tweet):\n",
    "    \n",
    "    #only read ascii english characters. Get rid od smiley unicode and other languages\n",
    "    tweet = filter(lambda x: x in string.printable, tweet)\n",
    "    \n",
    "    #remove newline characters\n",
    "    tweet = re.sub(\"\\n\",\"\", tweet)\n",
    "    \n",
    "    tweet_no_punct = re.sub('[^a-zA-Z0-9\\#]+', ' ', tweet)\n",
    "    \n",
    "    words = [word for word in tweet_no_punct.split()]\n",
    "    \n",
    "    #remove stopwords \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add(\"http\")\n",
    "    stop_words.add(\"the\")\n",
    "    meaningful_words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    #remove words less than 2 characters\n",
    "    long_words = [w for w in meaningful_words if len(w) > 2]\n",
    "    \n",
    "    return long_words\n",
    "\n",
    "\n",
    "def getWordsFromTweets(tweets):\n",
    "    tokenized_tweets = []\n",
    "    for tweet in tweets:\n",
    "        words = cleanAndTokenizeTweet(tweet)\n",
    "        tokenized_tweets.append(words)\n",
    "    return tokenized_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizedtweets = getWordsFromTweets(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['American',\n",
       " 'Sniper',\n",
       " 'doesn',\n",
       " 'make',\n",
       " 'proud',\n",
       " 'American',\n",
       " 'probably',\n",
       " 'need',\n",
       " 'check',\n",
       " 'pulse',\n",
       " 'Merica',\n",
       " 'd2bkKzbfWA']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizedtweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets write function for splitting a hasthtag into meaningful words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By close observation of hashtags, hashtags are either of these forms - \n",
    "1. Camel Case\n",
    "2. separated by underscore\n",
    "3. lowercase word sequence\n",
    "\n",
    "In case of 1 and 2, extracting words is simple. function hashtagToWords does the same. But in case of 3, we will use a different approach since it is not trivial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split the hashtags all in lowercase like this -> \"imissmyfriends\", we can look up into dictionary to see for valid words and decide split positions using like http://www.geeksforgeeks.org/dynamic-programming-set-32-word-break-problem/\n",
    "\n",
    "But how do we decide which split is the best. We need some basline corpus to decide which words are more frequent or appear together. Then we can rank all splits based on their probability. As it happens there is already a algorithm for doing this called Viterbi Algorithm. Lets see how this algorithm works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#uncomment to see code, or look in viterbi.py\n",
    "# %load ../viterbi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from viterbi import viterbi_segment\n",
    "segments, probs = viterbi_segment(\"imissmyfriends\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words:['i', 'miss', 'my', 'friends']\n",
      "probability:2.02391305979e-13\n"
     ]
    }
   ],
   "source": [
    "print \"words:\"+str(segments)\n",
    "print \"probability:\"+str(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_cap_re = re.compile('(.)([A-Z][a-z]+|[0-9]+)')\n",
    "all_cap_re = re.compile('([a-z0-9])([A-Z])')\n",
    "            \n",
    "def hashtagToWords(word):\n",
    "    words = []\n",
    "    if \"_\" in word:\n",
    "        words = word.split(\"_\")\n",
    "    elif all(x.isupper() for x in word):\n",
    "        words.append(word)\n",
    "    elif any(x.isupper() for x in word):\n",
    "        s1 = first_cap_re.sub(r'\\1 \\2', word)\n",
    "        words = all_cap_re.sub(r'\\1 \\2', s1).split()\n",
    "    else:\n",
    "        segs, prob = viterbi_segment(word)\n",
    "        words.extend(segs)\n",
    "    \n",
    "    hashtag_formatted = cleanAndTokenizeTweet(' '.join(words).lower())\n",
    "    \n",
    "    return hashtag_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american', 'sniper']\n",
      "['imax']\n",
      "['bradley', 'cooper']\n",
      "['american', 'sniper']\n",
      "['oscars', '2015']\n",
      "['birdman']\n",
      "['days', 'summer']\n",
      "['box', 'office']\n",
      "['miss']\n"
     ]
    }
   ],
   "source": [
    "print hashtagToWords(\"AmericanSniper\")\n",
    "print hashtagToWords(\"IMAX\")\n",
    "print hashtagToWords(\"BradleyCooper's\")\n",
    "print hashtagToWords(\"AmericanSniper:\")\n",
    "print hashtagToWords(\"Oscars2015\")\n",
    "print hashtagToWords(\"Birdman\")\n",
    "print hashtagToWords(\"The12DaysOfSummer\")\n",
    "print hashtagToWords(\"boxoffice\")\n",
    "print hashtagToWords(\"imissthis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Awesome!!!* We are done with task 1. And we have tested the function for all cases. Lets move of on to task 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to replace hashtags with split words.\n",
    "def getFormattedTweets(tokenizedtweets):\n",
    "    formatted_tweets = []\n",
    "\n",
    "    for item in tokenizedtweets:\n",
    "        templist = []\n",
    "        for word in item:\n",
    "            if word.startswith('#'):\n",
    "                templist.extend(hashtagToWords(word[1:]))\n",
    "            else:\n",
    "                templist.append(word)\n",
    "        \n",
    "        formatted_tweets.append(' '.join(templist).lower())\n",
    "    \n",
    "    return formatted_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'american sniper doesn make proud american probably need check pulse merica d2bkkzbfwa'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_tweets = getFormattedTweets(tokenizedtweets)\n",
    "formatted_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bradley Cooper tweets:4494\n",
      "Clint Eastwood tweets:3514\n",
      "Chris Kyle tweets:5366\n"
     ]
    }
   ],
   "source": [
    "bradley_cooper_tweets = filter(lambda x: \"bradley cooper\" in x, formatted_tweets)\n",
    "clint_eastwood_tweets = filter(lambda x: \"clint eastwood\" in x, formatted_tweets)\n",
    "chris_kyle_tweets = filter(lambda x: \"chris kyle\" in x, formatted_tweets)\n",
    "print \"Bradley Cooper tweets:\"+ str(len(bradley_cooper_tweets))\n",
    "print \"Clint Eastwood tweets:\"+ str(len(clint_eastwood_tweets))\n",
    "print \"Chris Kyle tweets:\"+ str(len(chris_kyle_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ultimate respect chris kyle 1974 2013 rip solider american sniper xu3woykitv',\n",
       " 'american sniper chris kyle widow this movie kids remember dad 3xoenmufuz lftadfk4nt',\n",
       " 'retweet support chris kyle 100 american sniper t6dnbzhlyl',\n",
       " 'intvd bradley cooper clint eastwood taya kyle sienna miller jason hall today chris kyle american sniper 8dipuwx5vq',\n",
       " 'american sniper chris kyle widow how coping xbysnjfryz 5mwacveorz']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chris_kyle_tweets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out top trending topics in a given set of tweets, there are number of approaches. We will explore only one of them which is very simple. Here are steps - \n",
    "\n",
    "1. Get the text corpus for matching tweets given a keyword.\n",
    "2. Vectorize the corpus using either Word-count or TF-IDF. (We will choose whichever gives best results).\n",
    "3. Club together synonyms and similar words/grams.\n",
    "4. Get top 5 words/grams with highest value of TF or TF-IDF.\n",
    "\n",
    "Lets only consider 1-gram and 2-gram only for words.\n",
    "For finding similar words and phrases, we will use wordnet dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tfidf \n",
    "import numpy as np\n",
    "#build term frequency-inverse document frequency matrix\n",
    "def getTFIDF(input_tweets):\n",
    "    td_dict, vocab = tfidf.tc(input_tweets, tokenizer=tfidf.get_bigrams)\n",
    "    td = tfidf.to_sparse_matrix(td_dict, vocab).toarray()\n",
    "    idf = tfidf.to_vector(tfidf.idf_from_tc(td_dict), vocab)\n",
    "    idc = tfidf.idc_from_tc(td_dict)\n",
    "    tf_by_idf = tfidf.tf_mul_idf(td, idf)\n",
    "    \n",
    "    #sum all tf-idf for a word and sort to get most important\n",
    "    tfmatrix = tf_by_idf.sum(axis=1)\n",
    "    sortedmatrix = np.argsort(tfmatrix, axis=0, )\n",
    "    inv_vocab = tfidf.inverse_vocab(vocab)\n",
    "    \n",
    "    results = [(inv_vocab[x], tfmatrix[x]) for x in sortedmatrix[-100:]]\n",
    "    results_dict = dict(results)\n",
    "    return results_dict, idc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Well, we have got some interesting results. Some observations on this -\n",
    "\n",
    "1. Some concepts are similar and hence results are repetitive. For ex, best, actor, best actor all represent same meaning\n",
    "2. We have ignored synonyms till now. But as we see in results above, a lot of synonyms can be clubbed together to represent a topic. For ex, amazing, great, sensational, etc\n",
    "3. One more thing. Clint Eastwood, Chris Kyle, Bradley Cooper happen to be the most used words in all tweets. So our results are always showing them as important. \n",
    "\n",
    "Lets tackle these issues one by one.\n",
    "First for every group of similar concepts, we will choose a bigram and average value of count among them.\n",
    "For ex, \n",
    "'chris kyle, chris, kyle' will come together as ('chris kyle', 520+565+651/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py:2507: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  VisibleDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "bradley_dict, bradley_idc = getTFIDF(bradley_cooper_tweets)\n",
    "clint_dict, clint_idc = getTFIDF(clint_eastwood_tweets)\n",
    "kyle_dict, kyle_idc = getTFIDF(chris_kyle_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getFinalResultDict(in_dict, in_idc):\n",
    "    final_dict = {}\n",
    "    bigram_words = []\n",
    "\n",
    "    for key, value in in_dict.iteritems():\n",
    "        one_grams = key.split()\n",
    "        fsum = value\n",
    "        counter = 1\n",
    "        if len(one_grams)==2:\n",
    "            if one_grams[0] in in_dict:\n",
    "                fsum = fsum + in_dict[one_grams[0]]\n",
    "                counter = counter + 1\n",
    "            if one_grams[1] in in_dict:\n",
    "                fsum = fsum + in_dict[one_grams[1]]\n",
    "                counter = counter + 1\n",
    "            final_dict[key] = (fsum / counter, in_idc[key])\n",
    "            bigram_words.append(one_grams[0])\n",
    "            bigram_words.append(one_grams[1])\n",
    "\n",
    "    for key, value in in_dict.iteritems():\n",
    "        one_grams = key.split()\n",
    "        if len(one_grams)==1:\n",
    "            if one_grams[0] not in bigram_words:\n",
    "                final_dict[key] = (value, in_idc[key])\n",
    "    \n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent Topics for Bradley Cooper\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(4021, 'american sniper'),\n",
       " (520, 'chris kyle'),\n",
       " (484, 'cooper american'),\n",
       " (413, 'americansniper'),\n",
       " (395, 'clint eastwood'),\n",
       " (372, 'sniper bradley'),\n",
       " (271, 'film'),\n",
       " (228, 'good'),\n",
       " (210, 'performance'),\n",
       " (196, 'great'),\n",
       " (195, 'navy seal'),\n",
       " (187, 'the'),\n",
       " (158, 'premiere'),\n",
       " (149, 'new'),\n",
       " (149, 'cooper sensational'),\n",
       " (141, 'portrayal navy'),\n",
       " (141, 'eastwood bradley'),\n",
       " (138, 'bradley coopers'),\n",
       " (130, 'see american'),\n",
       " (126, 'watch')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bradley_final_dict = getFinalResultDict(bradley_dict, bradley_idc)\n",
    "print \"Most frequent Topics for Bradley Cooper\"\n",
    "sorted( ((v[1],k) for k,v in bradley_final_dict.iteritems()), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent Topics for Clint Eastwood\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3304, 'american sniper'),\n",
       " (2040, 'clint eastwood'),\n",
       " (1462, 'clint eastwoods'),\n",
       " (1386, 'review clint'),\n",
       " (1383, 'movie american'),\n",
       " (1376, 'movie review'),\n",
       " (1373, 'sniper movie'),\n",
       " (1364, 'book interpretation'),\n",
       " (1363, 'eastwoods book'),\n",
       " (400, 'bradley cooper')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clint_final_dict = getFinalResultDict(clint_dict, clint_idc)\n",
    "print \"Most frequent Topics for Clint Eastwood\"\n",
    "sorted( ((v[1],k) for k,v in clint_final_dict.iteritems()), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kyle_final_dict = getFinalResultDict(kyle_dict, kyle_idc)\n",
    "print \"Most frequent Topics for Chris Kyle\"\n",
    "sorted( ((v[1],k) for k,v in kyle_final_dict.iteritems()), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, lets find out how unsuperived topic extraction using Non-Negative Matrix Factorization works from scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "seal sensational navy portrayal chris\n",
      "()\n",
      "Topic #1:\n",
      "response claims anti coopers american\n",
      "()\n",
      "Topic #2:\n",
      "best oscar actor oscars 2015\n",
      "()\n",
      "Topic #3:\n",
      "talks sniper american veterans news\n",
      "()\n",
      "Topic #4:\n",
      "eastwood clint featurette new talk\n",
      "()\n",
      "Topic #5:\n",
      "good amazing job looks film\n",
      "()\n",
      "Topic #6:\n",
      "movie great americansniper star awesome\n",
      "()\n",
      "Topic #7:\n",
      "performance eerie inside news american\n",
      "()\n",
      "Topic #8:\n",
      "sniper american watching film premiere\n",
      "()\n",
      "Topic #9:\n",
      "kyle chris taya americansniper miller\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "n_topics = 10\n",
    "n_top_words = 5\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                             stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(bradley_cooper_tweets)\n",
    "nmf = NMF(n_components=n_topics, random_state=1).fit(tfidf)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are pretty impressive and each group of word is clearly representing an idea. But this is not what is aksed for the task, so we will not use this method. This is just shown for illustration purposes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
