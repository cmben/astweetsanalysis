{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task is to break down hashtags into proper words. #GoldenGlobe -> Golden, Globe\n",
    "Let's first bring the data into a list. Each line/tweet is a item in a list. Further, we will treat them as a document comprising of some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadTweetsInList(filename):\n",
    "    tweets = open(filename, \"rU\")\n",
    "    message = tweets.readlines()\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = loadTweetsInList(\"../as.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['American Sniper\\n',\n",
       " \"If American Sniper doesn't make you proud to be an American, you probably need to check your pulse. Merica. http://t.co/d2bkKzbfWA\\n\",\n",
       " \"Retweet if you're going to see the movie American Sniper http://t.co/8Q93NiAWJJ\\n\",\n",
       " \"Retweet if you're going to see the movie American Sniper http://t.co/AQAtAo3vep\\n\",\n",
       " \"Retweet if you're going to see the movie American Sniper http://t.co/CYdwIZxcSW\\n\",\n",
       " \"Fav if you're going to see the movie American Sniper! http://t.co/DxWI1Sq6Jw\\n\",\n",
       " 'I just watched American Sniper. You have to see what Bradley Cooper does in this movie. His performance is next level.\\n',\n",
       " 'Did you know #Jesseventura sued fallen seal estate #AmericanSniper for $1.8M. You worthless piece of \\xf0\\x9f\\x92\\xa9. Shameless.\\n',\n",
       " \"American Sniper kind of reminds me of the movie that's showing in the third act of Inglorious Basterds.\\n\",\n",
       " 'Did y\\xe2\\x80\\x99all see \"American Sniper\"? What a powerful movie. Bradley Cooper isn\\xe2\\x80\\x99t too hard on the eyes, either.\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets drop first item since its just a title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = tweets[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets tokenize each string in this list. The result should be list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def cleanAndTokenizeTweet(tweet):\n",
    "    \n",
    "    #only read ascii english characters. Get rid od smiley unicode and other languages\n",
    "    tweet = filter(lambda x: x in string.printable, tweet)\n",
    "    \n",
    "    #remove newline characters\n",
    "    tweet = re.sub(\"\\n\",\"\", tweet)\n",
    "    \n",
    "    words = [word for word in tweet.split()]\n",
    "    \n",
    "    #remove stopwords \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    meaningful_words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    #remove words less than 2 characters\n",
    "    long_words = [w for w in meaningful_words if len(w) > 2]\n",
    "    \n",
    "    return long_words\n",
    "\n",
    "\n",
    "def getWordsFromTweets(tweets):\n",
    "    tokenized_tweets = []\n",
    "    for tweet in tweets:\n",
    "        words = cleanAndTokenizeTweet(tweet)\n",
    "        tokenized_tweets.append(words)\n",
    "    return tokenized_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizedtweets = getWordsFromTweets(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['American',\n",
       " 'Sniper',\n",
       " \"doesn't\",\n",
       " 'make',\n",
       " 'proud',\n",
       " 'American,',\n",
       " 'probably',\n",
       " 'need',\n",
       " 'check',\n",
       " 'pulse.',\n",
       " 'Merica.',\n",
       " 'http://t.co/d2bkKzbfWA']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizedtweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtags=[]\n",
    "for item in tokenizedtweets:\n",
    "    for word in item:\n",
    "        if word.startswith('#'):\n",
    "            hashtags.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#Jesseventura',\n",
       " '#AmericanSniper',\n",
       " '#AmericanSniper',\n",
       " '#AmericanSniper',\n",
       " '#AmericanSniper',\n",
       " '#The12DaysOfContent',\n",
       " '#AmericanSniper',\n",
       " '#AmericanSniper',\n",
       " '#AmericanSniper',\n",
       " '#AmericanSniper']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtags[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets write function for splitting a hasthtag into meaningful words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By close observation of hashtags, hashtags are either of these forms - \n",
    "1. Camel Case\n",
    "2. separated by underscore\n",
    "3. lowercase word sequence\n",
    "\n",
    "In case of 1 and 2, extracting words is simple. function hashtagToWords does the same. But in case of 3, we will use a different approach since it is not trivial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split the hashtags all in lowercase like this -> \"imissmyfriends\", we can look up into dictionary to see for valid words and decide split positions using like http://www.geeksforgeeks.org/dynamic-programming-set-32-word-break-problem/\n",
    "\n",
    "But how do we decide which split is the best. We need some basline corpus to decide which words are more frequent or appear together. Then we can rank all splits based on their probability. As it happens there is already a algorithm for doing this called Viterbi Algorithm. Lets see how this algorithm works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#uncomment to see code, or look in viterbi.py\n",
    "# %load ../viterbi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from viterbi import viterbi_segment\n",
    "segments, probs = viterbi_segment(\"imissmyfriends\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words:['i', 'miss', 'my', 'friends']\n",
      "probability:2.02391305979e-13\n"
     ]
    }
   ],
   "source": [
    "print \"words:\"+str(segments)\n",
    "print \"probability:\"+str(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_cap_re = re.compile('(.)([A-Z][a-z]+|[0-9]+)')\n",
    "all_cap_re = re.compile('([a-z0-9])([A-Z])')\n",
    "            \n",
    "def hashtagToWords(word):\n",
    "    words = []\n",
    "    if \"_\" in word:\n",
    "        words = word.split(\"_\")\n",
    "    elif all(x.isupper() for x in word):\n",
    "        words.append(word)\n",
    "    elif any(x.isupper() for x in word):\n",
    "        s1 = first_cap_re.sub(r'\\1 \\2', word)\n",
    "        words = all_cap_re.sub(r'\\1 \\2', s1).split()\n",
    "    else:\n",
    "        segs, prob = viterbi_segment(word)\n",
    "        words.extend(segs)\n",
    "    return words\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['American', 'Sniper']\n",
      "['IMAX']\n",
      "['Bradley', \"Cooper's\"]\n",
      "['American', 'Sniper:']\n",
      "['Oscars', '2015']\n",
      "['Birdman']\n",
      "['The', '12', 'Days', 'Of', 'Summer']\n",
      "['box', 'office']\n",
      "['i', 'miss', 'this']\n"
     ]
    }
   ],
   "source": [
    "print hashtagToWords(\"AmericanSniper\")\n",
    "print hashtagToWords(\"IMAX\")\n",
    "print hashtagToWords(\"BradleyCooper's\")\n",
    "print hashtagToWords(\"AmericanSniper:\")\n",
    "print hashtagToWords(\"Oscars2015\")\n",
    "print hashtagToWords(\"Birdman\")\n",
    "print hashtagToWords(\"The12DaysOfSummer\")\n",
    "print hashtagToWords(\"boxoffice\")\n",
    "print hashtagToWords(\"imissthis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Awesome!!!* We are done with task 1. And we have tested the function for all cases. Lets move of on to task 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to replace hashtags with split words.\n",
    "formatted_tweets = []\n",
    "\n",
    "for item in tokenizedtweets:\n",
    "    templist = []\n",
    "    for word in item:\n",
    "        if word.startswith('#'):\n",
    "            templist.extend(hashtagToWords(word[1:]))\n",
    "        else:\n",
    "            templist.append(word)\n",
    "    formatted_tweets.append(templist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Did',\n",
       " 'know',\n",
       " 'Jesseventura',\n",
       " 'sued',\n",
       " 'fallen',\n",
       " 'seal',\n",
       " 'estate',\n",
       " 'American',\n",
       " 'Sniper',\n",
       " '$1.8M.',\n",
       " 'You',\n",
       " 'worthless',\n",
       " 'piece',\n",
       " 'Shameless.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_tweets[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out top trending topics in a given set of tweets, there are number of approaches. We will explore only one of them which is very simple. Here are steps - \n",
    "\n",
    "1. Get the text corpus for matching tweets given a keyword.\n",
    "2. Vectorize the corpus using either Word-count or TF-IDF. (We will choose whichever gives best results).\n",
    "3. Club together synonyms and similar words/grams.\n",
    "4. Get top 5 words/grams with highest value of TF or TF-IDF.\n",
    "\n",
    "Lets only consider 1-gram and 2-gram only for words.\n",
    "For finding similar words and phrases, we will use wordnet dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import itertools\n",
    "totallist = list(itertools.chain.from_iterable(formatted_tweets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = nltk.Text(totallist[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American Sniper; you're going; Bradley Cooper; going see; Retweet\n",
      "you're; american sniper; 'American Sniper'; January 16th?; box office;\n",
      "come January; didn't come; (to use; see movie; Sniper didn't; American\n",
      "Sniper!; American Sniper.; thought American; movie American; saw\n",
      "American; see American\n"
     ]
    }
   ],
   "source": [
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
